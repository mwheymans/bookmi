---
output:
  html_document: default
  pdf_document: default
---
# (PART) Part VII: Background information to Multiple Imputation Methods {-}

# Rubin's Rules 

Rubin´s Rules (RR) are designed to pool parameter estimates, such as mean differences, regression coefficients, standard errors and to derive confidence intervals and p-values. 

We illustrate RR with a t-test example in 3 generated multiple imputed datasets in SPSS. The t-test is used to estimate the difference in mean Tampascale values between patients with and without Radiation in the leg. The output of the t-test in the multiple imputed data is presented in Figure \@ref(fig:tab9-1) and Figure \@ref(fig:tab9-2).

```{r tab9-1, echo = FALSE, fig.cap="T-test for difference in mean Tampascale values between patients with and without Radiation in the leg applied in multiple imputed datasets.", out.width='90%', fig.align='center'}
knitr::include_graphics("images/table5.1.png")
```

```{r tab9-2, echo = FALSE, fig.cap="T-test for difference in mean Tampascale values between patients with and without Radiation in the leg applied in multiple imputed datasets.", out.width='90%', fig.align='center'}
knitr::include_graphics("images/table5.1b.png")
```

The result in the original dataset (including missing values) is presented in the row that is indicated by Imputation_ number 0. Results in each imputed dataset are shown in the rows starting with number 1 to 3. In the last row which is indicated as “Pooled”, the summary estimates of the mean differences and standard errors are presented. We now explain how these pooled mean differences and standard errors are estimated using RR.

## Pooling Effect estimates

When RR are used, it is assumed that the repeated parameter estimates are normally distributed. This cannot be assumed for all statistical test statistics, e.g. correlation coefficients. For these test statistics, transformations are first performed before RR can be applied. 

To calculate the pooled mean differences of Figure \@ref(fig:tab9-1) the following formula is used \@ref(eq:p-param):

\begin{equation}
  \bar{\theta} = \frac{1}{m}\left (\sum_{i=1}^m{\theta_i}\right )
  (\#eq:p-param)
\end{equation}

In this formula, $\bar{\theta}$ is the pooled parameter estimate, m is the number of imputed datasets, $\theta_i$ means taking the sum of the parameter estimate (i.e. mean difference) in each imputed dataset i. This formula is equal to the basic formula of taking the mean value of a sequence of numbers. 

When we use the values for the mean differences in Figure \@ref(fig:tab9-1), we get the following result for the pooled mean difference:

$$\bar{\theta} = \frac{1}{3}(2.174 + 1.965+1.774)=1.971$$

## Pooling Standard errors

The pooled standard error is derived from different components that reflect the within and between sampling variance of the mean difference in the multiple imputed datasets. The calculation of these components is discussed below.

*Within imputation variance*
This is the average of the mean of the within variance estimate, i.e. squared standard error, in each imputed dataset. This reflects the sampling variance, i.e. the precision of the parameter of interest in each completed dataset. This value will be large in small samples and small in large samples.

\begin{equation}
V_W = \frac{1}{m} \sum_{i=1}^m{SE_i^2}
  (\#eq:var-w)
\end{equation}

In this formula $V_W$ is the within imputation variance, m is the number of imputed datasets, $SE_i^2$ means taking the sum of the squared Standard Error (SE), estimated in each imputed dataset i. Using the standard error estimates in Figure \@ref(fig:tab9-1), we get the following result:

$$V_W = \frac{1}{3}(0.896^2 + 0.882^2 + 0.898^2)=0.7957147$$

*Between imputation variance*
This reflects the extra variance due to the missing data. This is estimated by taking the variance of the parameter of interest estimated over imputed datasets. This formula is equal to the formula for the (sample) variance which is commonly used in statistics. This value is large when the level of missing data is high and smaller when the level of missing data is small.

\begin{equation}
V_B = \frac{\sum_{i=1}^m (\theta_i - \overline{\theta})^2}{m-1} 
  (\#eq:var-b)
\end{equation}

In this formula, $V_B$ is the between imputation variance, m is the number of imputed datasets, $\overline{\theta}$ is the pooled estimate, $\theta_i$ is the parameter estimate in each imputed dataset i. 

Using the mean differences in Figure \@ref(fig:tab9-1), we get the following result:

$$V_B = \frac{1}{3-1}((2.174-1.971)^2+ (1.965-1.971)^2+(1.774-1.971)^2)=0.040027$$
\begin{equation}
V_{Total} = V_W + V_B + \frac{V_B}{m}
  (\#eq:var-t)
\end{equation}

$$V_{Total} = 0.7957147+0.040027 + \frac{0.040027}{3}$$

$$SE_{Pooled} = \sqrt{V_{Total}} = \sqrt{0.849084} = 0.9214575$$

This value is equal to the (rounded) pooled standard error value of 0.921 in Figure \@ref(fig:tab9-1).

## Significance testing

For significance testing of the pooled parameter, i.e. the mean difference in Figure \@ref(fig:tab9-1), Formula \@ref(eq:wald-pooled) is used. This is the univariate Wald test (@Rubin1987, @VanBuuren2018, @Marshall2009MedResMeth). This test is defined as:

\begin{equation}
Wald_{Pooled} =\frac{(\overline{\theta} - {\theta_0})^2}{V_T}
  (\#eq:wald-pooled)
\end{equation}

where $\overline{\theta}$ is the pooled mean difference and $V_T$ is the total variance and is equal to the $SE_{Pooled}$ (pooled standard error) that was derived in the previous paragraph, and $\theta_0$ is the parameter value under the null hypothesis (which is mostly 0). The univariate Wald test can be used to test all kind of univariate parameters of interest, such as mean differences and univariate regression coefficients from different tpe of regression models. The univariate Wald test in our example is calculated using the pooled regression coefficient and standard error from Figure \@ref(fig:tab9-1):

$$Wald_{Pooled} = \frac{1.971}{\sqrt{0.849084}}=2.139$$

The univariate pooled Wald value follows a t-distribution. This distribution is used to derive the p-value. The value for t depends on the degrees of freedom, according to:

\begin{equation}
t_{df,1-\alpha/2}
  (\#eq:t-distr)
\end{equation}

Where df are the degrees of freedom and $\alpha$ is the reference level of significance, which is usually set at 5%. The derivation of the degrees of freedom for the t-test is complex. There exist different formula´s to calculate the degrees of freedom and these are explained in the next paragraph.

Because $t^2$ is equal to F at the same number of degrees of freedom, we can also test for significance using a F-distribution, according to:

\begin{equation}
F_{1, df}=t^2_{df,1-\alpha/2}
  (\#eq:f-distr)
\end{equation}

The degrees of freedom are equal to the degrees of freedom for the t-test above. 

## Degrees of Freedom and P-values 

The derivation of the degrees of freedom (df) and the p-value for the pooled t-test is not straightforward, because there are different formulas to calculate the df, an older and an adjusted version (@VanBuuren2018). The older method to calculate the dfs results in a higher value for the  df's for the pooled result than the one in each imputed dataset. An example can be found in Figure \@ref(fig:tab9-3). The degrees of freedoms are 148 in each imputed dataset (in the row for equal variances assumed, under the column df) and 507 for the pooled result. This is important to relize because different values for the df's lead to different p-values. 
In SPSS the old way to calculate the dfs is used. Adjusted versions are used in the mice package for R. The differences between the older and adjusted methods to calculate the df's is illustrated in more detail below. 

```{r tab9-3, echo = FALSE, fig.cap="Part of Output of Figure 5.1. The value for the dfs are presented in the df column.", out.width='90%', fig.align='center'}
knitr::include_graphics("images/table5.2.png")
```

The (older method) to calculate the df for the t-distribution is defined as (@Rubin1987, @VanBuuren2018):

\begin{equation}
df_{Old} = \frac{m-1}{lambda^2} = (m-1) * (1 + \frac{1}{r})^2
  (\#eq:df-old)
\end{equation}

Where m is the number of imputed datasets and lambda is equal to the Fraction of Missing information (FMI), calculated by Formula \@ref(eq:lambda) (@raghunathan2016), and r is the relative increase in variance due to nonresponse (RIV), calculated by Formula \@ref(eq:riv). 

The lambda value that is used in Formula \@ref(eq:df-old) (and often used as alternative for the FMI) is not the same value for the fraction of missing information of 0.067 that SPSS presents in Figure \@ref(fig:tab9-1). The FMI value of 0.067 is calculated by Formula \@ref(eq:fmi) and is called FMI. 

When $df_{old}$ is calculated with the information in Figure \@ref(fig:tab9-3), we get:

$$df_{Old} = \frac{3-1}{0.06283485^2} = 506.5576$$

This (rounded) value is equal to the df value in the row Pooled of 507 in (Figure \@ref(fig:tab9-1)). Formula \@ref(eq:df-old) leads to a larger df for the pooled result, compared to the dfs in each imputed dataset, which is inappropriate. Therefore, @BARNARD1999 adjusted this df by using Formula \@ref(eq:df-adj):

\begin{equation}
df_{Adjusted} = \frac{df_{Old}*{df_{Observed}}}{df_{Old}+{df_{Observed}}}
  (\#eq:df-adj)
\end{equation}

Where $df_{Old}$ is defined as in Formula \@ref(eq:df-old) and $df_{Observed}$ is defined as:

\begin{equation}
df_{Observed} = \frac{(n-k)+1}{(n-k)+3}*(n-k)(1-lambda)
  (\#eq:df-obs)
\end{equation}

Where n is the sample size in the imputed dataset, k the number of parameters to fit and lambda is obtained by Formula Formula \@ref(eq:lambda).

By filling in the formulas \@ref(eq:df-obs) and \@ref(eq:df-adj) we get for $df_{observed}$ and $df_{adjusted}$ respectively:

$$df_{Observed} = \frac{(150-2)+1}{(150-2)+3}*(150-2)(1- 0.06283485)=136.8633$$

$$df_{Adjusted} = \frac{(506.5576* 136.8633)}{(506.5576+ 136.8633)}=107.7509$$

The number of 107.7509 is equal to the df used by mice.

We can now derive the p-value for the pooled mean difference in the Tampascale between patients with and without Radiation in the leg. This two-sided p-value is:

In SPSS:

$$t_{df,1-\alpha/2}=2.139_{df{Old}}=0.03289185$$

In R:

$$t_{df,1-\alpha/2}=2.139_{df{Adjusted}}=0.03467225$$

## Confidence Intervals

For the 95% confidence interval (CI), the general formula can be used:

\begin{equation}
\bar{\theta} ± t_{df,1-\alpha/2} * SE_{Pooled}
  (\#eq:conf)
\end{equation}

In this formula, $\bar{\theta}$ is the pooled estimate, t is the t-statistic, df is degrees of freedom and $SE_{Pooled}$ is the pooled standard error (Formula \@ref(eq:var-t)). 

# Measures of Missing data information 

These measures are the Fraction of Missing information (FMI), the relative increase in variance due to nonresponse (RIV) and the Relative Efficiency (RE). They are derived from values of the between, and within imputation variance and the total variance. There exist two versions of the FMI, which are referred to as lambda and FMI.

## Fraction of Missing Information - Lambda

The proportion of total variance due to missingness, lambda, (@VanBuuren2018; @raghunathan2016) can be derived from the between and total missing data variance as: 

\begin{equation}
Lambda = \frac{V_B + \frac{V_B}{m}}{V_T}
  (\#eq:lambda)
\end{equation}

Where m is the number of imputed datasets and ${V_B}$ and ${V_T}$ are the between and total variance respectively. This value can be interpreted as the proportion of variation in the parameter of interest due to the missing data.

When we use the ${V_B}$ and ${V_T}$ values that were calculated in paragraph 5.1.2, lambda will be:

$$Lambda = \frac{0.040027 + \frac{0.040027}{3}}{0.849084}=0.06283485$$

This specific value for lambda is not reported by SPSS, but is reported by the mice package in R. @VanBuuren2018 and @enders2010applied use the same formula to calculate this type of missing data information, but van Buuren calls it lambda and Enders FMI.

## Relative increase in variance

Another related measure is the relative increase in variance  due to nonresponse. This value is calculated as:

\begin{equation}
RIV = \frac{V_B + \frac{V_B}{m}}{V_W}
  (\#eq:riv)
\end{equation}

Where ${V_B}$ and ${V_W}$ are the between and within variance respectively. This value can be interpreted as the proportional increase in the sampling variance of the parameter of interest that is due to the missing data.

Filling in this formula with the values for ${V_B}$ and ${V_W}$ from paragraph 5.1.2 results in:


$$RIV = \frac{0.040027 + \frac{0.040027}{3}}{0.7957147}=0.06704779$$

This value is also presented in (Figure \@ref(fig:tab9-1)) in the column Relative Increase Variance. The relation between RIV and Lambda is defined as

\begin{equation}
RIV = \frac{Lambda }{1 - Lambda}.
  (\#eq:rivlambda)
\end{equation}



## Fraction of Missing Information - FMI

\begin{equation}
FMI = \frac{RIV + \frac{2}{df+3}}{1+RIV}
  (\#eq:fmi)
\end{equation}

Where RIV is the relative increase in variance due to missing data and df is the degrees of freedom for the pooled result. The degrees of freedom for the pooled result can be obtained in two ways: ${df_{Old}}$ or ${df_{Adjusted}}$. 

In SPSS, FMI is calculated using ${df_{Old}}$, which results in:

$$FMI = \frac{RIV + \frac{2}{df+3}}{1+RIV}=\frac{0.06704779 + \frac{2}{506.5576+3}}{1+0.06704779}=0.0665132$$

In R package mice, FMI is calculated using the formula for ${df_{Adjusted}}$, that results in:

$$FMI = \frac{RIV + \frac{2}{df_{Adjusted}+3}}{1+RIV}=\frac{0.06704779 + \frac{2}{107.7509+3}}{1+0.06704779}=0.0797587$$

The difference between lambda and FMI is that FMI is adjusted for the fact that the number of imputed datasets that are generated is not unlimitedly large. These measures differ for a small value of the df.

## Relative Efficiency

The Relative Efficiency (RE) is defined as:

\begin{equation}
RE = \frac{1}{1+\frac{FMI}{m}}
  (\#eq:re)
\end{equation}

FMI is the fraction of missing information and m is the number of imputed datasets.

The RE value is only provided by SPSS and is calculated by filling in the values of (Figure \@ref(fig:tab9-1)) as follows:

$$RE = \frac{1}{1+\frac{0.0665132}{3}}=0.9783098$$

The RE gives information about the precision of the parameter estimate as the standard error of a regression coefficient.

# Pooling correlation coefficients

To pool correlation coefficients Fishers Z transformation is used. The following formulas are used (@raghunathan2016, @VanBuuren2018 and @enders2010applied):

\begin{equation}
Z_i = \frac{1}{2}ln\frac{1+r_i}{1-r_i}
  (\#eq:cor)
\end{equation}

The ${Z_i}$ means the calculation of Fisher's Z-value in each imputed dataset.

Also, the variance of the correlation can be calculated using:

\begin{equation}
Var_Z=\frac{1}{n-3}
  (\#eq:var-cor)
\end{equation}

n is the sample size in the imputed dataset. Now we can use Rubin's Rules to calculate the Pooled correlation and variance. These values will be calculated with the transformed Z values.

To obtain the pooled p-value for the correlation coefficient we use the formula:

\begin{equation}
Z=\frac{Z_{Pooled}}{\sqrt{Var_Z}} = \frac{Z_{Pooled}}{\frac{1}{\sqrt{n-3}}}=Z_{Pooled}\times\sqrt{n_i-3}
  (\#eq:z-cor)
\end{equation}

In this formula z is the z-score and follows a standard normal distribution, $Z_{Pooled}$ is the pooled Z transformation and $Var_Z$ is the pooled variance.

Finally, back transformation to the original scale of r is done by:

\begin{equation}
r_{Pooled} = \frac{e^{2\times\\Z_{Pooled}}-1}{e^{2\times\\Z_{Pooled}}+1}
  (\#eq:exp-cor)
\end{equation}

# Pooled Wald test

The significance level for the pooled OR is derived by using the pooled Wald test. The pooled Wald test is calculated as:

$$Wald_{Pooled} =\frac{-0.067}{0.046}=$$

This Wald pooled value follows a t-distribution with degrees of freedom (df) according to Formula 5.9.

$$df_{Old} = \frac{m-1}{lamda^2}$$

For this Formula we need information of lambda, which is calculated as:

$$lambda = \frac{V_B + \frac{V_B}{m}}{lamda^2}$$

Using the values of the regression coefficients and standard errors, estimated in each imputed dataset of (Figure \@ref(fig:tab5-9)) we can calculate the following values for the between imputation and the total variance.

$$V_B= \frac{(-0.090+0.067)^2 + (-0.061+0.067)^2 +(-0.051+0.067)^2}{2}=\frac{0.000821}{2}=0.0004105$$

To calculate the total variance also the within imputation variance is needed. The within imputation variance can be calculated using Formula 5.2:

$$V_W= \frac{0.039^2 + 0.040^2 + 0.039^2}{3}=0.001547333$$

The total variance becomes:

$$V_{Total} = 0.001547333+0.0004105+ \frac{0.0004105}{3}=0.002094666$$

Now we can calculate lambda using:

$$lambda = \frac{0.0004105 + \frac{0.0004105}{m}}{0.002094666}=0.2612986$$

The lambda value is not presented by SPSS, but only in R using mice. Now we know the value for lambda, we can calculate the degrees of freedom to derive the p-value:

$$df_{Old} = \frac{m-1}{lamda^2}=\frac{2}{0.2612986^2}=29.29246$$

Which results in a p-value of: 0.1502045.

# Pooling Methods for Categorical variables

## The pooled sampling variance or D1 method

Alternatively, a combination of the pooled parameter estimates and the pooled sampling variances can be used to construct a test that resembles a multivariate Wald test (Marshall, 2009). This test pools within and between covariance matrices of each imputed dataset and finally corrects the total parameter covariance matrix of the multivariate Wald test by including the average relative increase in variance to account for the missing data. 

The multivariate Wald statistic is calculated as (Enders, 2010; Marshall et al., 2009):

\begin{equation}
  D_1 = \frac{ (\bar\theta - \theta_0)V_T^{-1} (\bar\theta - \theta_0) } {k}
  (\#eq:d1)
\end{equation}
 
where $\bar\theta$ and $theta_0$ are the pooled coefficient and the value under the null hypothesis (mostly zero), $V_T$ is the total variance, and k is the number of parameters. $V_T$ is:
 
 $$V_T = (1+r_{1})V_W$$
 
$r_1$ is the relative increase in variance due to nonresponse (fraction of missing information), which is in this case obtained by:

\begin{equation}
\bar r_1 = \left(1+\frac{1}{m}\right)\mathrm{tr}(V_B\bar V_W^{-1})/k
(\#eq:r1)
\end{equation}

where $V_B$ is the between imputation variance, $V_W$ the within imputation variance, m the number of imputed datasets. 

The p-value of the $D_1$ statistic is calculated by comparing the value to an F distribution with $k$ and $v_1$ degrees of freedom. 

$$p = \Pr[F_{k,\nu_1}>D_1]$$

\begin{equation}
v_1 = 4 + (t-4)[1+(1-2t^{-1})r_1^{-1}]^2
(\#eq:v1a)
\end{equation}

this equation is used when $t = k(m-1) > 4$, otherwise use:

\begin{equation}
v_1 = t(1+k^{-1})(1+r_1^{-1})^2/2 
(\#eq:v1b)
\end{equation}

## Multiple parameter Wald test or D2 method

One possibility is to pool the Chi-square values from the multiple parameter Wald or likelihood ratio tests with multiple degrees of freedom. This procedure is also called the D2 procedure (Enders 2012). We used this procedure also in the previous Chapter to obtain the pooled Chi-square values. 

The following formula is used to obtain the Chi-square values from a multiple parameter Wald test (Marshall, 2009): 

\begin{equation}
 D_2 = (1+r)^{-1} (\frac{ \bar\omega}{k}-\frac{ m+1}{m-1}r)
  (\#eq:d2)
\end{equation}

where $$\bar\omega$$ is the mean of the Chi-square values over the imputed datasets, k is the degrees of freedom of the Chi-square test statistic, m is the number of imputed datasets and r reflects the relative increase in variance due to nonresponse, which is obtained by the following formula:

$$r_2 = \left(\frac{m+1}{m(m-1)}\right)\sum_{j=1}^m\left(\sqrt{\omega_j}-{\sqrt{\bar\omega}}\right)^2$$

with m and $\bar\omega$ as above,  j = 1 …, m  the index of  each separate imputed dataset and $\omega_j$ is the Chi-square value in each imputed dataset. The p-value is calculated by comparing the $D_2$ statistic to an F distribution with k and v  degrees of freedom as follows:

$$p = \Pr[F_{k,\nu_1}>D_2]$$

The application of the function to pool Chi-square values can be found in the R code below where the function `miPoolChi` is applied. Input values in the function are the Chi-square values of each imputed dataset that are shown in Table 6.1 and the degrees of freedom of the Chi-square test, which is 2 here.

```{r}
miceadds::micombine.chisquare(c(1.815,
            1.303,
            2.826,
            1.759,
            3.634),2)

```

As we already showed in the previous Chapter this pooling function is also available in the `miceadds` package, which will of course provide the same results.

##Meng and Rubin pooling

Meng and Rubin proposed a method to test overall categorical variables indirectly based on the likelihood ratio test statistic (MR pooling) (@Meng1992, @Mistler2013). For each regression parameter, two nested models are fitted in each imputed dataset: one restricted model where the parameter is not included in the model and one full model where the parameter is included. The pooled likelihood ratio tests are then compared to obtain pooled p-values for each parameter. The MR pooling method requires fitting multiple models for each variable in the data, hence it is an indirect approach. This can be a very time-consuming process. 

Meng and Rubin pooling (MR pooling)
The Meng and Rubin pooling method (also called $D_3$ method) works according to the following steps (@Meng1992): 
1. For each regression parameter $\theta$ two nested models are fitted in each imputed dataset: one where $\theta$  is included (full model) and one where $\theta$ is not included in the model (restricted model). Subsequently, these models are pooled to obtain $\bar\theta_{full}$ and $\bar\theta_{restricted}$.

2. The average likelihood ratio test statistic $\bar d_L$ over the imputed datasets as a result of comparing the log likelihood values between these models is calculated as:
	
3.	
	
$$\bar d = \frac{1}{m} \sum_{j=1}^m 2(L_{restricted} - L_{full})$$

where $L_{restricted}$ and $L_{full}$ represent the maximum log likelihood values with respect to $\theta$.

4. The log likelihood values from the two models of step 2 are then re-calculated  and averaged using the model parameters $\bar\theta_{full}$ and $\bar\theta_{restricted}$ of step 1 (which were constrained to the values from the models in the imputed data): 
	
$$\bar d_{constrained} = \frac{1}{m} \sum_{j=1}^m 2(L(\bar\theta_{full} - \bar\theta_{restricted})$$

5. The resulting test statistic $D_L$, required to obtain the pooled p-value, is calculated by incorporating the average increase in variance due to nonresponse $\bar r_L$ as follows:

$$D_3 = \frac{\bar d_{constrained}}{k(1+\bar r_L)}$$

$$\bar r_L = \frac{m+1}{k(m-1)}(\bar d_L-\bar d_{constrained})$$

where k is the number of degrees of freedom in the complete data likelihood ratio test (@Mistler2013; @VanBuuren2018). The p-value is calculated by comparing  the $D_3$ statistic with an F distribution with k and $v_L$  (i.e., degrees of freedom of the denominator) according to:

$$p = \Pr[F_{k,\nu_L}>D_3]$$

with:

$$v_L = 4 + (km-k-4)[1+(1-\frac{2}{(km-k)}\frac{1}{(r_L)}]^2$$


## The Median P Rule

For the Median P Rule (MPR) one simply uses the median p-value of the significance tests conducted in each imputed dataset (MPR pooling). Hence, it depends on p-values only and not on the parameter estimates. The MPR can be calculated by using the p-values from the likelihood ratio test for multiple parameters for the categorical variables in the multivariable model (@Eekhout2017). The Median P Rule is therefore very simple to apply. For the median p-value to be valid you have to run the MI procedure without the outcome variable in the imputation model.
