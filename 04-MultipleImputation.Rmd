---
output:
  html_document: default
  pdf_document: default
---

# (PART) Part III: Multiple Imputation {-}

# Multiple Imputation

In this Chapter we discuss an advanced missing data handling method, that is called Multiple Imputation (MI). With MI, each missing value is replaced by several different values and consequently several different completed datasets are generated. The concept of MI can be made clear by the following figure \@ref(fig:fig4-1).

```{r fig4-1, echo = FALSE, fig.cap="Graphical presentation of the MI procedure.", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.1.png")
```

In the first step, the dataset with missing values (i.e. the incomplete dataset) is copied several times. Then in the next step, the missing values are replaced with imputed values in each copy of the dataset. In each copy, slightly different values are imputed due to random variation. This results in mulitple imputed datasets. In the third step, the imputed datasets are each analyzed and the study results are then pooled into the final study result. In this Chapter, the first phase in multiple imputation, the imputation step, is the main topic. In the next Chapter, the analysis and pooling phases are discussed. 

There are many different algorithms developed to impute missing values with a multiple imptuation procedure. For example multivariate imputation, where all variables are imputed in one iteration under the multivariate normal assumption, or multivariate imputation by chained equations, where variables are imputed sequentially. The latter method will be explained more thorougly in the next paragraph.

##Multivariate Imputation by Chained Equations

Multivariate imputation by chained equations (MICE) is also known as Sequential Regression Imputation, Fully Conditional Specification or Gibbs sampling. In the MICE algorithm, a chain of regression equations is used to obtain imputations, which means that variables with missing data are imputed one by one. The regression models use information from all other variables in the model, i.e. conditional imputation models. In order to add sampling variability to the imputations, residual error is added to create the imputed values. This residual error can either be added to the prediced values directly, which is esentially similar to repeating stochastic regression imputation over several imputation runs. Or, the residual variance can be added via the parameter estimates of the regression model, which is a Bayesian sampling method. The Bayesian method is the default in the `mice` package in R. 

###The mice algorithm and iteration steps 

Each imputed dataset is generated after several iterations of the imputation algorithm. The imputation algorithm includes the chain of regression equations to estimate the imputed values. How this works exactly is explained with the LBP data as an illustration. In this data we have missing values in the Tampa scale variable and the disability variable; the other two variables, radiation and pain, are completely observed.

Per imputed dataset we start with iteration number 0. Data points are randomly drawn from the observed values of the Tampa scale and the Disability variable and these are used to replace the missing values in these variables. 

For iteration 1 the Tampa scale values are set back to missing. Subsequently, a linear regression model is applied in the available data (i.e. all subjects with observed Tampa scale values) using the Tampa scale as the dependent variable and Pain, Disability and Radiation as independent variables. Note that for this regression the imputed values for disability from the previous iteration are used. The Baysian sampling method then draws regression coefficients from the posterior distribution of the parameters of this regression. The imputed values for Tampa scale are the predicted values from the linear regression. This regression equation is defined as: 

$Tampa_{mis} = \beta_0 + \beta_1Pain + \beta_2Disability + \beta_3Radiation$

where $\beta_0$, $\beta_1$, $\beta_2$ and $\beta_3$ are Bayseian sampling draws from their posterior distributions. The same procedure is repeated for the Disability variable. The Disability scores are first set back to missing, then the regression coefficients for the Pain, Tampa scale and Radiation variables are obtained from the subjects without missing diability values. Note that the imputed values for Tampa scale are used. The imputed values for disability are estimated using the regression coefficients with additional random error drawn from the residual error distribution. 

$Disability_{mis} = \beta_0 + \beta_1Pain + \beta_2Tampa + \beta_3Radiation$

For iteration 2 the Tampa scale values are again set back to missing and (new) updated regression coefficients for Pain, Disability and Radiation are obtained, using the imputed values for Disability from iteration 1. Accordingly, missing values are estimated from regression model where the coefficients are again drawn from the posterior distributions of the estimated parameters. The same holds for the Disability variable. The imputed values for disability are estimated by the regression model using the imputed values in Tampa scale from iteration 2. This process is repeated in each following iteration until the final iteration where the imputed values are used for the first imputed dataset. For the next imputed dataset, the entire process of iterations is repeated.

##Multiple Imputation in R

In R multiple imputation can be performed with the `mice` function from the `mice` package. The following default settings are used in the `mice` function to start MI, `m=5`, to generate 5 imputed datasets, `maxit=10`, to use 10 iterations for each imputed dataset (i.e. 10 chains of regression imputation models), `method=”pmm”`. For an elobate explanation of all options withing the mice function, see `?mice`.


```{r}
library(mice)
library(foreign)
data <- read.spss(file="Backpain50 MI missing.sav", to.data.frame=T)[, -1]
imp <- mice(data, m=5, maxit=10, method="pmm")
```

By default, the mice fucntion returns information about the iteration and imputation steps for the variable that are imputed under the columns named “iter”, “imp” and “variable”. This information can be turned off by setting the mice function parameter printFlag = FALSE, which results in silent computation of the missing values. A summary of the imputation results can be obtained by calling the imp object.

```{r}
imp
```

This object contains information about the algorithm, the number of imputed datasets, the missing values in each variable, the imputation method, the VisitSequence which is the order in which the variables are imputed during the imputation process, information of the PredictorMatrix and the seed value for the random number generator. The imputed datasets can be extracted by using the complete function. The setting `action = 1` returns the first imputed dataset. The settings `action = ”long”` and `include = TRUE` returns a data.frame where the imputed datasets are stacked under each other and include the original dataset (with missings) on top (see ?complete for more possibilities how to store the imputed datasets).

```{r}
complete(imp, action = 1)
complete(imp, action = "long", include = TRUE)
```

In the imputed datasets two variables are added, an .id variable and an .imp variable to distinguish the cases in the dataset and the imputed datasets. The imputed datasets can be further used in mice to conduct pooled analyses or to store them for further use in other software packages as SPSS. 

###Customizing the Imputation model

In the exemplar imputation model, the variables Tampa scale and Disability are imputed with the help of variables Pain and Radiation. The latter two variables are called auxiliary variables when they are not part of the main analysis model but they help to impute the Tampa scale and disability variables. Variables that are used to impute other variables can be switched off and on in the predictormatrix. 

```{r}
imp$PredictorMatrix
```

The predictor matrix is a matrix with the names of the variables in the dataset listed in the rows and the columns. The variables in the columns are used to impute the row variables. Accordingly, variables in the columns can be switched on or off to in- or exclude them from the imputation model to impute the missing data in the row variable. In our example, the first and fourth rows contain only zeroes, because the Pain and Radiation variables do not have missing values and therefore do not need to be imputed. The variable in the second row, i.e. Tampa scale, contains missing values and the 1´s in this row mean that the column variables Pain, Disability and Radiation are included in the imputation model. For the Disability variable, the variables Pain, Tampa scale and Radiation are used. As a default setting all variables are included in the imputation model to predict missing values in other variables. The diagonal of the predictormatrix is always zero. The predictormatrix can be adapted when for example a variable that contains a high percentage of missing data should be excluded from the imputation model to impute other variables. For example, if we want to exclude the variable Disability from the imputation model of the Tampa scale variable we can do the following:

```{r, eval=FALSE}
pred <-imp$PredictorMatrix
pred["Tampascale", "Disability"] <- 0
pred

imp2 <- mice(data,m=5, maxit=10, method="pmm", predictorMatrix = pred, seed=1050)
```

In literature several guidelines for the imputation model are described (@Collins2001, @VanBuuren2018, @Rubin1976). A summary of guidelines for building the imputation model is as follows:

1. Include all variables that are part of the analysis model, including the dependent (outcome) variable.
2. Include the variables at the same way in the imputation model as they appear in the analysis model (i.e. if interaction terms are in the analysis model they also have to be included in the imputation model).
3.	Include additional (auxiliary) variables that are related to missingness or to variables with missing values.

##Output of the `mice` function

The mice function returns a mids (multiple imputed data set) object. In this object, aimputation information is stored and can be extracted by typing `imp$`, followed by the type of information you want to obtain. 

```{r}
imp$m
imp$nmis
imp$seed
imp$iteration
```
The above objects contain the the number of imputed datasets, missing values in each variable, the specified seed value and the number of iterations.
The original data can be found in:
```{r}
imp$data
```
The imputed values for each variable in the imptued values can be found under:
```{r}
imp$imp
```
The imputation methods used:
```{r}
imp$method
```

The predictor matrix:
```{r}
imp$predictorMatrix
```

The sequence of the variables used in the impution procedure:
```{r}
imp$visitSequence
```

###Checking Convergence in R

The convergence of the imputation procedure can be evaluates. The means of the imputed values for each iteration can be extracted from the mids object as chainMean. The number of Chains is equal to the number of imputed datasets. A Chain refers to the chain of regression models that is used to generate the imputed values. The length of each chain is equal to the number of iterations. The Chains contain the means of the imputed values. 
```{r}
imp$chainMean
```

The convergence can be visualised by plotting the chain information in a convergence plot. For our example, the convergence plots are shown below. In this plot you see that the variance between the imputation chains is almost equal to the variance within the chains, which indicates healthy convergence. 


```{r}
plot(imp)
```

###Imputation diagnostics in R

It can also be of interest to compare the values that are imputed with those that are observed. For that, the `stripplot` function can be used in `mice`. This function visualises the observed and imputed values in one plot. By comparing the observed and the imputed data points we get an idea if the imputed values are in range of the observed data. If there are no large differences between the imputed and observed values under MAR than we can conclude the imputed values are plausible.

```{r}
stripplot(imp)

```

##Multiple imputation in SPSS

The multiple imputation procedure in SPSS is based on the MICE algorithm that was developed in R. It is therefore no surprise that the settings of the mice function in R, are almost all covered by the options in SPSS. Before you start the MI procedure it is important to set the measurement level of the variables with missing data in the Variable View window of your data. They are important for the regression model that is used to estimate the missing values in that variable. For example, if you define a variable as scale, then linear regression models are used, for categorical variables, logistic regression models are used. 

###Multiple imputation in SPSS

Before we start the multiple imputation procedure, we set the starting point of the random number generator in SPSS at a fixed value of 950 (in R we use the seed for this). In this way we are able to reproduce results exactly. This fixing of starting values for the random number generator is in general not advisable, we use it here for educational purposes.

We set the random number generator in SPSS via 

>Transform -> Random Number Generators -> Set Starting point -> Fixed Value

```{r fig4-5, echo = FALSE, fig.cap="Set the Random Number Generator ", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.5.png")

```

The multiple imputation procedure in SPSS can be started via 

>Analyze -> Multiple Imputation -> Impute Missing Data Values.

```{r fig4-6, echo = FALSE, fig.cap="The variables Tab", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.6.png")

```

In the Variables tab the variables that are part of the imputation model have to be transported to the window “Variables in Model”. The variables are imputed sequentially in the order in which they are listed in the variables list. These variables are in our example the Pain, Tampascale, Disability and Radiation variables. Further, the number of imputed datasets can be defined as well as the dataset to which the imputed data values are saved. We choose for 5 imputations and call the dataset in which the imputed values will be stored “LBP_Imp”. In the Methods Tab the imputation method is defined.

```{r fig4-7, echo = FALSE, fig.cap="The Methods Tab", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.7.png")

```

In the Method tab we choose for a “Custom” under Imputation Method and for Fully conditional specification. The Fully Conditional Specification (FCS) procedure is the Bayesian sequential regression imputation method as explained in *section 4.3 and 4.4*. Under Model type for scale (continuous) variables we choose for Predictive Mean Matching. This procedure was explained in *section 3.7*. In the `mice` package PMM is the default procedure for continuous variables with missing data. In SPSS the default is the linear regression procedure. In the Constraints Tab the options per variable are defined.

```{r fig4-8, echo = FALSE, fig.cap="The Constraints Tab", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.8.png")

```

In the Constraints tab the role of a variable during the imputation process is defined and it is possible to restrict the range of imputed values of a scale variable. In addition, you can restrict the analysis to variables with less than a maximum percentage of missing values. When the PMM method is selected in the Method Tab, the Constraints tab can be skipped. Finally, in the Output Tab the generated output can be defined.

```{r fig4-9, echo = FALSE, fig.cap="The Output Tab", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.9.png")

```

Display the Imputation model and the Descriptive statistics for variables with imputed values options can be selected for information about the imputation. When the FCS imputation method is used, you can request a dataset that contains iteration history data for FCS imputation. In our example this iteration history information is stored in the dataset “Iter_Backpain”. The dataset contains means and standard deviations by iteration for each scale variable for which values are imputed. You can plot the data to help assess model convergence.

###The output for Multiple imputation in SPSS

After the multiple imputation procedure is done, a new data window opens that contains the imputed datasets. These multiple imputed datasets are stacked on top of each other. In this file the imputed values are marked yellow. There is also an extra variable added to the file which is called `Imputation_` (Figure \@ref(fig:fig4-10)). The imputed values can be marked and unmarked via

> View -> Mark Imputed data

If you switch this possibility on, SPSS automatically recognizes the dataset as a multiple imputed dataset. If you switch this possibility off, SPSS treats the dataset as one dataset. This marking and unmarking can also be done in the Data view window via the button with yellow and white squares on the right site above (Figure \@ref(fig:fig4-11)). If you click the button, a selection box appears with “Original data” selected,where you can easily move to the different imputed datasets. 
The variable `Imputation_` is a variable with a nominal measure. You can compare the use of this variable with the Split File option in SPSS where all analyses are done separately for the categories of the variable use to split the analyses. The difference is that with the `Imputation_` variable you also obtain pooled estimates for the statistical analyses. 

```{r fig4-10, echo = FALSE, fig.cap="Example of Multiple Imputed dataset", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.10.png")

```

```{r fig4-11, echo = FALSE, fig.cap="Button and selection box to mark imputed values", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.11.png")

```

The iteration history is stored in the Iter_Backpain dataset as we defined in the Output window. In this dataset the means and standard deviations of the imputed values at each iteration are stored. These values can be used to construct Convergence plots. More about making convergence plots will be discussed in the next paragraph.

```{r fig4-12, echo = FALSE, fig.cap="The iteration history data", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.12.png")

```

Based on our settings, SPSS produces the following results in the output window. In the Imputation Specifications table, information is provided on the imputation method used, the number of imputations, the model used for the scale variables, if interactions were included in the imputation models, the setting for the maximum percentage of missing values and the setting for the maximum number of parameters in the imputation model (Figure \@ref(fig:tab4-4)).

```{r tab4-4, echo = FALSE, fig.cap="Imputation Specifications table", out.width='90%', fig.align='center'}
knitr::include_graphics("images/tab4.4.png")

```

A second table, called Imputation Results, is presented with information about the imputation method, the number of fully conditional specification methods, the variables that are imputed and not imputed and the imputation sequence(Figure \@ref(fig:tab4-5)).

```{r tab4-5, echo = FALSE, fig.cap="Imputation Results", out.width='90%', fig.align='center'}
knitr::include_graphics("images/tab4.5.png")

```

The Imputation Models table presents information about the imputation models used for the variables with missing data (Figure \@ref(fig:tab4-6)). Information is provided about the method of imputation, under the type column, the effect estimates used to impute the missing values, the number of missing and imputed values. For example for the Tampascale variable 13 values were missing and m=5 times 13 is 65 values were imputed.

```{r tab4-6, echo = FALSE, fig.cap="Imputation Models", out.width='90%', fig.align='center'}
knitr::include_graphics("images/tab4.6.png")

```

The Descriptive statistics display the descriptive information of the original, imputed and completed data of the Tampascale and the Disability variable. In this way you can compare the completed data after MI with the original data.

```{r tab4-7, echo = FALSE, fig.cap="Descriptive statistics", out.width='90%', fig.align='center'}
knitr::include_graphics("images/tab4.7.png")
knitr::include_graphics("images/tab4.8.png")

```

###Checking Convergence after Multiple imputation in SPSS

The dataset Iter_Backpain in the previous paragraph contains the means and standard deviations of the imputed values at each iteration and imputation round. This information is similar as the 
information in `imp$chainMean` in R. This dataset can be used to generate convergence plots, to check if the imputed values have the expected variation between the iterations.The iteration can be checked for the means and standard deviations seperately. In order to obtain seperate plots for htese summary statistics, the split file option in SPSS can be activated. 

```{r fig4-17, echo = FALSE, fig.cap="Split file", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.17.png")

```

After activation of the split file option, the Graph menu in SPSS can be used to make the plots. 

>Graph -> Char Builder.

```{r fig4-13, echo = FALSE, fig.cap="Graph menu", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.13.png")

```

Two windows will open that can be used to build a chart:

```{r fig4-14, echo = FALSE, fig.cap="Chart Builder", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.14.png")

```

On the x-axis the put the `iteration number` variable and on the y-axis the variable for which we want to display the iteration history. The `Imputation Number` variable is dragged to the `set color` top-right.   

```{r fig4-15, echo = FALSE, fig.cap="Chart Builder", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.15.png")

```

As a result two plots appear with the iteation history for each imputation run. 

```{r fig4-18, echo = FALSE, fig.cap="Convergence plots", out.width='90%', fig.align='center'}
knitr::include_graphics("images/fig4.18.png")
knitr::include_graphics("images/fig4.19.png")

```

## Predictive Mean Matching or Regression imputation

Within the mice algorithm continuous variables can be imputed by two methods, linear regression imputation or Predictive Mean Matching (PMM). PMM is an imputation method that predicts values and subsequently selects observed values to be used to replace the missing values. We recommend to use PMM during imputation. It is the default imputation procedure in the mice package [@Rubin1987]. In SPSS the default imputation procedure is linear regression. 

### Predictive Mean Matching, how does it work?

The Predictive Mean Matching algorithm takes place in several steps:

We take as an example a dataset with 10 cases with 3 missing values in the Tampa scale variable. They are defined as `NA` in the dataset below. The Pain variable is used to predict the missing Tampa scale values.

```{r echo=FALSE}

library(foreign)
dataset <- suppressWarnings(suppressMessages(read.spss(file="PMM 10 missing.sav", to.data.frame = T)))
dataset

```

**Step 1**: Estimate a linear regression model 

A linear regression model is estimated with the Tampa scale variable as the outcome and the Pain variable as the predictor variable. We define the regression coefficient for Pain as $\hat{\beta_{Pain}}$.

**Step 2**: Determine Bayesion version of regression coefficient 

A Bayesian regression coefficient for the Pain variable is determined. We define this regression coefficient as $\beta_{Pain}^*$.

**Step 3**: Predict Missing values

_Observed_ Tampa scale valueas are predicted by the Pain regression coefficient $\hat{\beta_{Pain}}$ from step 1 and the Pain data, we call these values $Tampa_{Obs}$ and can be found in the Table below.

```{r , echo=FALSE, eval=FALSE}
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(kableExtra)))
suppressWarnings(suppressMessages(library(mice)))
suppressWarnings(suppressMessages(library(float)))
```

```{r , echo=FALSE}
library(float)
library(foreign)
library(mice)
library(knitr)
library(kableExtra)
dataset <- suppressWarnings(suppressMessages(read.spss(file="PMM 10 missing.sav", to.data.frame = T)))

y <- dataset$Tampascale
ry <- !is.na(y)

x <- as.matrix(dataset[, 1])
dimnames(x) <- list(NULL, "Pain")

donors = 3
type = 1
ridge = 1e-05

  x <- cbind(1, as.matrix(x))
  ynum <- y

  set.seed(3281)
  parm <- .norm.draw(ynum, ry, x, ridge = ridge)
  
  yhatobs <- x[ry, ] %*% parm$coef
  yhatmis <- x[!ry, ] %*% parm$beta
  
y[is.na(y)] <- yhatmis  
x[ry, ] <- yhatobs
x[!ry, ] <- yhatmis
dataset <- data.frame(dataset, x[, 1], ry, y)

# predicted missing value = 1, predicted observed value =0
# Prepare dataset to make plot
dataset$ry <- ifelse(dataset$ry, 0, 1)

names(dataset) <- c("ID", "Pain", "Tampascale", "Tampa_Obs", "Group", "Tampascale_Complete")

dataset <- dataset[, -c(5, 6)]
dataset <- round(dataset, 3)
dataset$Tampa_Obs[which(is.na(dataset$Tampascale))] <- NA

kable(dataset, "latex", booktabs = T)
```

_Missing_ Tampa scale valueas are predicted by the regression coefficient $\beta_{Pain}^*$ from step 2 and the Pain data, we call these $Tampa_{Pred}$.

These values for the three missing Tampa scale are: 43.594, 41.456 and 39.852.

**Step 4**: Find closest donor

Find the closest donor for the first missing value by subtracting the first $Tampa_{Pred}$ value of 43.594 from all predicted observed values in the $Tampa_{Obs}$ column. These differences are shown in the column Difference in the table below. 

```{r , echo=FALSE}
library(foreign)
library(mice)
dataset <- suppressWarnings(suppressMessages(read.spss(file="PMM 10 missing.sav", to.data.frame = T)))

y <- dataset$Tampascale

ry <- !is.na(y)

x <- as.matrix(dataset[, 1])
dimnames(x) <- list(NULL, "Pain")

donors = 3
type = 1
ridge = 1e-05

  x <- cbind(1, as.matrix(x))
  ynum <- y

  set.seed(3281)
  parm <- .norm.draw(ynum, ry, x, ridge = ridge)
  
  yhatobs <- x[ry, ] %*% parm$coef
  yhatmis <- x[!ry, ] %*% parm$beta
  
y[is.na(y)] <- yhatmis  
x[ry, ] <- yhatobs
x[!ry, ] <- yhatmis
dataset <- data.frame(dataset, x[, 1], ry, y)

# predicted missing value = 1, predicted observed value =0
# Prepare dataset to make plot
dataset$ry <- ifelse(dataset$ry, 0, 1)

names(dataset) <- c("ID", "Pain", "Tampascale", "Tampa_Obs", "Group", "Tampascale_Complete")

dataset <- dataset[, -c(5, 6)]
#dataset$Col <- ry
dataset <- round(dataset, 3)
dataset$Tampa_Obs[which(is.na(dataset$Tampascale))] <- NA

z <- as.array(yhatmis)
yhat = yhatobs
y = y[ry]
donors = donors

# subtract first missing value from all predicted values of observed data
dataset$Difference <- abs(round(dataset$Tampa_Obs, 3)-43.594)
dataset <- round(dataset, 3)

kable(dataset, "html")
```

The smallest differences are 1.574, 1.970 and 2.168 and these belong to the cases with observed Tampa scale values of 40, 41 and 42 respectively. Subsequently, a value is randomly drawn from these observed values and used to impute the first missing Tampa scale value. Other missing values are imputed by following the same procedure, i.e. now subtracting the second $Tampa_{Pred}$ value of 41.456 from all predicted observed alues and finding the closest match. 

The strength of PMM is that missing data is replaced by data that is observed in the dataset and not replaced by unrealistic values (as negative Tampa scale scores). PMM can therefore handle better the imputation of variables with skewed distributions or non-linear relationships between variables.   

##Number of Imputed datasets and iterations

Researchers assume that the number of imputations needed to generate valid imputations has to be set at 3-5 imputations. This idea was based on the work of Rubin (@Rubin1987). He showed that the precision of a pooled parameter becomes lower when a finite number of multiply imputed datasets is used compared to an infinite number (finite means a limited number of imputed datasets, like 5 imputed datasets and infinite means unlimited and can be recognized by the mathematical symbol ∞). The precision of a parameter is often represented by the sampling variance (or standard error (SE) estimate; the sampling variance is equal to SE2) of for example a regression coefficient. In case of multiple imputed datasets precision is determined by the pooled sampling variance or pooled SE. A measure to value the amount of precision (i.e. between the pooled sampling variance estimated in a finite compared to an infinite number of imputed datasets) is the relative efficiency ($RE$). The $RE$ is low when the number of imputations is high (and the precision becomes larger) and is defined as:


$$RE=  \frac{1}{1+ \frac{FMI}{m}}$$

FMI is the fraction of missing information and m is the number of imputed datasets. Where FMI is roughly equal to the percentage of missing data in the simplest case of one variable with missing data. When there are more variables in the imputation model, and these variables are correlated with the variables with missing data the FMI becomes lower.

The relationship between the $RE$ and the pooled sampling variance ($T_) can be written as (@VanBuuren2018):
$$T_{Pooled,finite}=RE×T_{Pooled,infinite}$$
which is equal to:
$$SE_{Pooled,finite}^2=RE×SE_{Pooled,infinite}^2 $$

These can be interpreted as follows: if the $RE$ is 0.93 for FMI=0.4 and m=5, $T_{Pooled,finite}$ is:

$$T_{Pooled,finite}=0.93×T_{Pooled,infinite}$$

Accordingly, when 5 imputed datasets are used, the standard error $SE$ is √0.93=0.96 times as large as the $SE$ when an infinite number of imputed datasets are used. Because the $RE$ is divided by 1, when 5 imputed datasets are used, the $SE$ is 1/√0.93=1.04 times larger (or 4%) than the $SE$ when an infinite number of imputed datasets are used. Graham (@Graham2007) also studied the loss in power when infinite numbers of imputed datasets are used. They recommended that at least 20 imputed datasets are needed to restrict the loss of power when testing a relationship between variables. Bodner (@Bodner2008) proposed the following guidelines after a simulation study using different values for the FMI to determine the number of imputed datasets. For FMI´s of 0.05, 0.1, 0.2, 0.3, 0.5 the following number of imputed dataets are needed: ≥3, 6, 12, 24, 59, respectively. Following the study of Bodner (@Bodner2008), White et al. (@White2011), proposed a rule of thumb, based on the idea that the FMI is frequently lower than the percentage of missing cases. Their rule of thumb states that the number of imputed datasets should be at least equal to the percentage of missing cases. This means that when 10% of the subjects have missing values, at least 10 imputed datasets should be generated.

Iterations
Van Buuren (@VanBuuren2018) states that the number of iterations may depend on the correlation between variables and the percentage of missing data in variables. He proposed that a number of 5-20 iterations is enough to reach convergence. This number may be adjusted when the percentage of missing data is high. Nowadays computers are fast so that a higher number of iterations can easily be used. 


